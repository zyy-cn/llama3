seed: 2024

num_epochs: 200
eval_interval: 1
eval_only: 0

lr_pow: 0.5
lr: 0.0001
beta1: 0.5
optimizer: adam
weight_decay: 0.0005

model:
  ckpt_dir: Meta-Llama-3-8B-Instruct
  tokenizer_path: Meta-Llama-3-8B-Instruct/tokenizer.model
  max_seq_len: 128
  max_gen_len: 0
  max_batch_size: 32
  top_p: 0.9
  temperature: 0.6
  DINOv2_BACKBONE_SIZE: giant
  text_embed_dim: 4096
dataset:
  num_workers: 4
  root_dir_mscoco: data/MSCOCO
  repeat: 0
  resize: 224
  train_batch_size: 32
  val_batch_size: 16
  split: val
  range_start: -1
  range_end: -1
  is_curated: 0